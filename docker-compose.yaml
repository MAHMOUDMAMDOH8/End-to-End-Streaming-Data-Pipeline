networks:
  BigData-network:
    name: BigData-network
    driver: bridge
  airflow-network:
    driver: bridge

services:
  # HDFS NameNode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=bigdata-cluster
    volumes:
      - hdfs-namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - BigData-network

  # HDFS DataNode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - hdfs-datanode:/hadoop/dfs/data
    networks:
      - BigData-network
    depends_on:
      - namenode

  # Spark (connects to HDFS)
  spark:
    build:
      context: ./spark
      dockerfile: dockerfile
    container_name: spark
    volumes:
      - ./includes:/opt/spark/includes
      - ./jinja_templates:/jinja_templates
      - ./environment.env:/environment.env
      - ./tests:/tests
      - ./Source:/opt/spark/source
      - ./spark/core-site.xml:/opt/spark/conf/core-site.xml  # Mount HDFS config
      - ./hadoop/bin:/opt/spark/hadoop/bin
      - ./spark-apps:/opt/spark-apps
      - ./Source:/opt/spark/Source
    ports:
      - 7077:7077  # Expose Spark master port
      - 8888:8888
      - 8080:8080
      - 10000:10000
      - 10001:10001
      - 2222:22
    networks:
      - BigData-network
    depends_on:
      - namenode
      - datanode

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181
    networks:
      - BigData-network

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
      - 29092:29092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - BigData-network

  # Airflow services (unchanged)
  airflow-webserver:
    container_name: airflow-webserver
    build: .
    env_file:
      - ./environment.env
    command: webserver
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./config:/opt/airflow/config
      - ./plugins:/opt/airflow/plugins
      - ./db:/opt/airflow/db
      - ./Scripts:/opt/airflow/Scripts
      - ./includes:/opt/airflow/includes
      - ./environment.env:/opt/airflow/environment.env
      - ./spark-apps:/opt/spark-apps
      - ./Source:/opt/airflow/Source
      - ./dbt:/opt/airflow/dbt
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:///opt/airflow/db/airflow.db
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__CORE__TEST_CONNECTION: 'Enabled'
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      AIRFLOW__WEBSERVER__SECRET_KEY: 1234
    user: "${AIRFLOW_UID:-50000}:0"
    networks:
      - airflow-network
      - BigData-network

  airflow-scheduler:
    container_name: airflow-scheduler
    build: .
    env_file:
      - ./environment.env
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./config:/opt/airflow/config
      - ./plugins:/opt/airflow/plugins
      - ./db:/opt/airflow/db
      - ./Scripts:/opt/airflow/Scripts
      - ./includes:/opt/airflow/includes
      - ./environment.env:/opt/airflow/environment.env
      - ./spark-apps:/opt/spark-apps
      - ./Source:/opt/airflow/Source
      - ./dbt:/opt/airflow/dbt
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/db/airflow.db
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__CORE__TEST_CONNECTION: 'Enabled'
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      AIRFLOW__WEBSERVER__SECRET_KEY: 1234
    user: "${AIRFLOW_UID:-50000}:0"
    networks:
      - airflow-network
      - BigData-network

  airflow-init:
    container_name: airflow-init
    build: .
    env_file:
      - ./environment.env
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /opt/airflow/{logs,dags,plugins,db}
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,db}
        exec /entrypoint airflow version && airflow config get-value database sql_alchemy_conn
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/db/airflow.db
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      AIRFLOW__WEBSERVER__SECRET_KEY: 1234
    user: "0:0"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./config:/opt/airflow/config
      - ./plugins:/opt/airflow/plugins
      - ./db:/opt/airflow/db
      - ./Scripts:/opt/airflow/Scripts
      - ./includes:/opt/airflow/includes
      - ./environment.env:/opt/airflow/environment.env
      - ./spark-apps:/opt/spark-apps  
      - ./Source:/opt/airflow/Source
      - ./dbt:/opt/airflow/dbt
    networks:
      - airflow-network
      - BigData-network

  airflow-cli:
    container_name: airflow-cli
    build: .
    env_file:
      - ./environment.env
    environment:
      - AIRFLOW__WEBSERVER__SECRET_KEY=1234
    command:
      - airflow
      - version
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./config:/opt/airflow/config
      - ./plugins:/opt/airflow/plugins
      - ./db:/opt/airflow/db
      - ./Scripts:/opt/airflow/Scripts
      - ./includes:/opt/airflow/includes
      - ./environment.env:/opt/airflow/environment.env
      - ./spark-apps:/opt/spark-apps
      - ./Source:/opt/airflow/Source
      - ./dbt:/opt/airflow/dbt
    networks:
      - airflow-network
      - BigData-network

volumes:
  hdfs-namenode:
  hdfs-datanode:
  airflow_logs:
  airflow_db:
